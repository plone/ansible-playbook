-----------------------------------
Test with the "multiserver" sample.
-----------------------------------

    >>> sample = 'sample-multiserver.yml'

    >>> extras = r"""
    ... admin_email: test@example.com
    ... plone_initial_password: admin
    ... additional_packages:
    ...   - curl
    ...   - lsof
    ... muninnode_query_ips:
    ...     - 127.0.0.1
    ... """


    >>> import subprocess
    >>> import sys
    >>> import time

Set up local-configure.yml by copying our sample.
Append admin email and password.

    >>> with open(sample, 'r') as f:
    ...     with open('local-configure.yml', 'w') as g:
    ...         g.write(f.read() + extras)

Vagrant up

    >>> print >> sys.stderr, "Bringing up %s" % box
    >>> run("vagrant up %s --no-provision" % box)

Vagrant provision -- unless contraindicated.

    >>> if skip_provisioning:
    ...     print >> sys.stderr, "Skipping provisioning"
    ... else:
    ...     print >> sys.stderr, "Provisioning"
    ...     run("vagrant provision %s" % box)

Add a couple of test host entries. The trick with `tee` allows us to append to a file with sudo.

    >>> command = "echo '127.0.0.1 test.example.com' | sudo tee -a /etc/hosts > /dev/null"
    >>> p = subprocess.call('vagrant ssh %s -c "%s"' % (box, command), shell=True)
    >>> command = "echo '127.0.0.1 test2.example.com' | sudo tee -a /etc/hosts > /dev/null"
    >>> p = subprocess.call('vagrant ssh %s -c "%s"' % (box, command), shell=True)

Reboot the box. We only care about what survives a restart.

    >>> if skip_restart:
    ...     print >> sys.stderr, "Skipping restart"
    ... else:
    ...     print >> sys.stderr, "Poweroff %s" % box
    ...     p = subprocess.call('vagrant ssh %s -c "sudo poweroff"' % box, shell=True)
    ...     time.sleep(30)
    ...     print >> sys.stderr, "Bringing %s back up" % box
    ...     run("vagrant up %s --no-provision" % box)
    ...     print >> sys.stderr, "Waiting for services to start..."
    ...     time.sleep(120)

And, now run tests against the box.

    >>> print >> sys.stderr, "Running tests against %s" % box

Check our motd.

    >>> print ssh_run('cat /etc/motd')
    This server is configured via Ansible.
    Do not change configuration settings directly.
    <BLANKLINE>
    Last Ansible provisioning: ...
    <BLANKLINE>
    Admin email: test@example.com
    <BLANKLINE>
    Custom Services/Ports
    ---------------------
    plone:
      - primary: /usr/local/plone-5.2/primary
        haproxy front end: 8080; zeo server: 127.0.0.1:8100; zeo clients: 127.0.0.1:8081 127.0.0.1:8082
        http://...; /Plone; aliases: default
        https://...; /Plone; aliases: default
      - secondary: /usr/local/plone-5.1/secondary
        haproxy front end: 7080; zeo server: 127.0.0.1:7100; zeo clients: 127.0.0.1:7081 127.0.0.1:7082
        http://localhost; /Plone
        http://test.example.com; /Plone
    haproxy monitor: http://127.0.0.1:1080/admin
    varnish: 127.0.0.1:6081
    varnish admin: 127.0.0.1:6082
    munin-node: 4949
    postfix: 25 (host-only)
    nginx: [80, 443]
    <BLANKLINE>

Use lsof to make sure we are listening on all expected ports
in all expected ways.

    >>> rez = ssh_run('sudo lsof -i4 -i6 -sTCP:LISTEN -Pn | tail -n +2')
    >>> listeners = sorted(set(cut(rez, [0, 2, 8])))

    >>> print "\n".join([s for s in listeners if 'plone_daemon' in s])
    python...   plone_daemon    127.0.0.1:7081
    python...   plone_daemon    127.0.0.1:7082
    python...   plone_daemon    127.0.0.1:7181
    python...   plone_daemon    127.0.0.1:7182
    python...     plone_daemon    127.0.0.1:8081
    python...     plone_daemon    127.0.0.1:8082
    zeoserver   plone_daemon    127.0.0.1:7100
    zeoserver   plone_daemon    127.0.0.1:8100...
    >>> print "\n".join([s for s in listeners if 'haproxy' in s])
    haproxy haproxy *:1080
    haproxy haproxy *:7080
    haproxy haproxy *:8080

    >>> listening_on = ['127.0.0.1:6082', '*:6081']
    >>> rez = "\n".join([s for s in listeners if 'varnishd' in s])
    >>> [s for s in listening_on if s not in rez]
    []

    >>> print "\n".join([s for s in listeners if 'nginx' in s])
    nginx   ... *:443
    nginx   ... *:80
    nginx   ... *:443
    nginx   ... *:80

    # smtpd
    >>> print "\n".join([s for s in listeners if ':25' in s])
    master  root    *:25...

    >>> print "\n".join([s for s in listeners if 'munin' in s])
    munin-nod  root *:4949

Check the process list.

    >>> print ssh_run('ps  -eo comm,euser:15,egroup | egrep "plone" | egrep -v "(egrep|systemd)" | sort')
    python...          plone_daemon    plone_group
    python...          plone_daemon    plone_group
    python...          plone_daemon    plone_group
    python...          plone_daemon    plone_group
    zeoserver       plone_daemon    plone_group
    zeoserver       plone_daemon    plone_group

Check supervisor's job list.

    >>> print joined_cut(ssh_run('sudo supervisorctl status'), [0, 1], True)
    primary_memmon                   RUNNING
    primary_zeoclient1               RUNNING
    primary_zeoclient2               RUNNING
    primary_zeoserver                RUNNING
    secondary_zeoclient1             RUNNING
    secondary_zeoclient2             RUNNING
    secondary_zeoserver              RUNNING


Primary instance tests
----------------------

Is everything where we expect it to be?

    >>> print joined_cut(ssh_run('ls -la /usr/local/plone-5.2 | tail -n +2'), [0, 2, 3, 8]).replace('.\t', '\t')
    drwxr-xr-x root           ...        .
    ...
    drwxr-xr-x plone_buildout plone_group buildout-cache
    drwxr-xr-x plone_buildout plone_group primary

    >>> print joined_cut(ssh_run('ls -la /usr/local/plone-5.2/primary | tail -n +4'), [0, 2, 3, 8]).replace('.\t', '\t')
    -rw-------  plone_buildout  plone_group     .installed.cfg
    drwxr-xr-x  plone_buildout  plone_group     bin
    -rw-r--r--  plone_buildout  plone_group     buildout.log
    drwxr-xr-x  plone_buildout  plone_group     develop-eggs...
    drwxr-xr-x  plone_buildout  plone_group     lib
    -rw-r--r--  plone_buildout  plone_group     live.cfg
    drwxr-xr-x  plone_buildout  plone_group     parts
    drwxr-xr-x  plone_buildout  plone_group     products...
    -r--r--r--  plone_buildout  plone_group     requirements.txt
    drwxr-xr-x  root    root    scripts...
    drwxr-xr-x  plone_buildout  plone_group     src
    drwxr-xr-x  plone_buildout  plone_group     var

    >>> print joined_cut(ssh_run('sudo ls -la /var/local/plone-5.2/ | tail -n +2'), [0, 2, 3, 8]).replace('.\t', '\t')
    drwxr-xr-x root         ...         .
    ...
    drwxrws--- plone_daemon plone_group primary

    >>> print joined_cut(ssh_run('sudo ls -la /var/local/plone-5.2/primary | tail -n +4'), [0, 2, 3, 8]).replace('.\t', '\t')
    drwx------  plone_daemon    plone_group .python-eggs
    drwxr-sr-x  plone_daemon    plone_group blobstorage
    drwxrws---  plone_buildout  plone_group cache
    drwxrws---  plone_buildout  plone_group client1
    drwxrws---  plone_buildout  plone_group client2
    drwxrws---  plone_buildout  plone_group client_reserved
    drwxrws---  plone_buildout  plone_group filestorage
    drwxrws---  plone_buildout  plone_group zeoserver

    >>> print joined_cut(ssh_run('sudo ls -l /var/local/plone-5.2/primary/filestorage | tail -n +2'), [0, 2, 3, 8]).replace('.\t', '\t')
    -rw-r--r--  plone_daemon    plone_group Data.fs
    -rw-r--r--  plone_daemon    plone_group Data.fs.index
    -rw-r--r--  plone_daemon    plone_group Data.fs.lock
    -rw-r--r--  plone_daemon    plone_group Data.fs.tmp

We should be able to get a page on port 80.

    >>> print ssh_run('curl --ipv4 -I -s http://%s | egrep "^(HTTP|Content-Type|X-Varnish-Cache)"' % box)
    HTTP/1.1 200 OK
    Content-Type: text/html;charset=utf-8
    X-Varnish-Cache: MISS

And, we should have gzip encoding available:

    >>> print ssh_run('curl --ipv4 -H "Accept-Encoding:gzip" -I -s http://%s | egrep "^(HTTP|Content-Encoding)"' % box)
    HTTP/1.1 200 OK
    Content-Encoding: gzip

Asking twice for a static resource should result in a cache hit.

    >>> print ssh_run('curl --ipv4 -I -s http://%s/logo.png > /dev/null' % box)
    >>> print ssh_run('curl --ipv4 -I -s http://%s/logo.png | egrep "^(HTTP|Content-Type|X-Varnish-Cache)"' % box)
    HTTP/1.1 200 OK
    Content-Type: image/png
    X-Varnish-Cache: HIT

SSL should work. Note that we're testing with a self-signed certificate:

    >>> print ssh_run('curl --ipv4 --insecure -I -s http://%s' % box)
    HTTP/1.1 200 OK
    ...

Let's prove to ourselves that this is Plone 5:

    >>> output = ssh_run('curl --ipv4 http://%s' % box)
    >>> output.find('barceloneta') >= 0
    True

Asking twice for a static resource should result in a cache hit.

    >>> print ssh_run('curl --ipv4 -I -s http://%s/logo.png > /dev/null' % box)
    >>> print ssh_run('curl --ipv4 -I -s http://%s/logo.png | egrep "^(HTTP|X-Varnish-Cache)"' % box)
    HTTP/1.1 200 OK
    X-Varnish-Cache: HIT

The restart script exercises our control of the supervisor
processes, haproxy and varnish.

    >>> print ssh_run('sudo /usr/local/plone-5.2/primary/scripts/restart_clients.sh')
    pre restart script running
    Marking client 1 down for maintenance
    Restarting client 1
    primary_zeoclient1: stopped
    Waiting a bit...
    primary_zeoclient1: started
    Waiting a bit...
    Fetch site homepages to warm cache
      /Plone
      /Plone
    Marking client 1 available
    ...
    post restart script running
    Done

The varnish cache for this host should now be empty.

    >>> print ssh_run('curl --ipv4 -I -s http://%s/logo.png | egrep "^(HTTP|X-Varnish-Cache)"' % box)
    HTTP/1.1 200 OK
    X-Varnish-Cache: MISS...

This instance should be using superlance's memmon for hot restarts.

    >>> print ssh_run('grep memmon /etc/supervisor/conf.d/primary_zeo.conf')
    [eventlistener:primary_memmon]
    command=memmon  -p primary_zeoclient1=750MB  -p primary_zeoclient2=750MB  -c -m root -n primary_

But not a cron job.

    >>> print ssh_run('grep "primary" /etc/cron.d/restart_hot_plones')
    #Ansible: Restart primary clients if too hot
    #15,45 * * * * root /usr/local/plone-5.2/primary/scripts/restart_if_hot.py

The load balancer for this instance should be running health checks.

    >>> print ssh_run('grep "8081" /etc/haproxy/haproxy.cfg')
    server client1 127.0.0.1:8081

And, have longer timeouts.

    >>> print ssh_run('grep "longer timeout for primary" /etc/haproxy/haproxy.cfg')
    timeout connect 30s  # longer timeout for primary

Secondary instance tests
------------------------

    >>> print joined_cut(ssh_run('ls -la /usr/local/plone-5.1 | tail -n +2'), [0, 2, 3, 8]).replace('.\t', '\t')
    drwxr-xr-x  root    ...     .
    ...                         ..
    drwxr-xr-x  plone_buildout  plone_group buildout-cache
    drwxr-xr-x  plone_buildout  plone_group secondary

    >>> print joined_cut(ssh_run('ls -la /usr/local/plone-5.1/secondary | tail -n +4'), [0, 2, 3, 8]).replace('.\t', '\t')
    -rw-------  plone_buildout  plone_group     .installed.cfg
    drwxr-xr-x  plone_buildout  plone_group     bin
    -rw-r--r--  plone_buildout  plone_group     buildout.cfg
    -rw-r--r--  plone_buildout  plone_group     buildout.log
    drwxr-xr-x  plone_buildout  plone_group     develop-eggs
    drwxr-xr-x  plone_buildout  plone_group     include
    drwxr-xr-x  plone_buildout  plone_group     lib...
    drwxr-xr-x  plone_buildout  plone_group     parts
    drwxr-xr-x  plone_buildout  plone_group     products...
    -r--r--r--  plone_buildout  plone_group     requirements.txt
    drwxr-xr-x  root    root    scripts...
    drwxr-xr-x  plone_buildout  plone_group     var

    >>> print joined_cut(ssh_run('sudo ls -la /var/local/plone-5.1 | tail -n +2'), [0, 2, 3, 8]).replace('.\t', '\t')
    drwxr-xr-x  root    ...   .
    ...                         ..
    drwxrws---  plone_daemon    plone_group secondary

    >>> print joined_cut(ssh_run('sudo ls -la /var/local/plone-5.1/secondary/ | tail -n +4'), [0, 2, 3, 8]).replace('.\t', '\t')
    drwx------  plone_daemon    plone_group     .python-eggs...
    drwxr-sr-x  plone_daemon    plone_group     blobstorage...
    drwxrws---  plone_buildout  plone_group     client1
    drwxrws---  plone_buildout  plone_group     client2
    drwxrws---  plone_buildout  plone_group     client_reserved
    drwxrws---  plone_buildout  plone_group     filestorage
    drwxrws---  plone_buildout  plone_group     zeoserver

    >>> print joined_cut(ssh_run('sudo ls -l /var/local/plone-5.1/secondary/filestorage | tail -n +2'), [0, 2, 3, 8]).replace('.\t', '\t')
    -rw-r--r--  plone_daemon    plone_group Data.fs
    -rw-r--r--  plone_daemon    plone_group Data.fs.index
    -rw-r--r--  plone_daemon    plone_group Data.fs.lock
    -rw-r--r--  plone_daemon    plone_group Data.fs.tmp


We should be able to get a page on port 80.

    >>> print ssh_run('curl --ipv4 -I -s http://localhost | egrep "^(HTTP|Content-Type|X-Varnish-Cache)"')
    HTTP/1.1 200 OK
    Content-Type: text/html;charset=utf-8
    X-Varnish-Cache: MISS

And, we should have gzip encoding available:

    >>> print ssh_run('curl --ipv4 -H "Accept-Encoding:gzip" -I -s http://localhost/ | egrep "^(HTTP|Content-Encoding)"')
    HTTP/1.1 200 OK
    Content-Encoding: gzip

Let's prove to ourselves that this is Plone 5ish:

    >>> output = ssh_run('curl --ipv4 http://localhost')
    >>> output.find('barceloneta') >= 0
    True

Asking for test.example.com should also produce a Plone 5ish page:

    >>> output = ssh_run('curl --ipv4 http://test.example.com')
    >>> output.find('barceloneta') >= 0
    True

Asking twice for a static resource should result in a cache hit.

    >>> print ssh_run('curl --ipv4 -I -s http://localhost/logo.png > /dev/null')
    >>> print ssh_run('curl --ipv4 -I -s http://localhost/logo.png | egrep "^(HTTP|Content-Type|X-Varnish-Cache)"')
    HTTP/1.1 200 OK
    Content-Type: image/png
    X-Varnish-Cache: HIT

The restart script exercises our control of the supervisor
processes, haproxy and varnish.

    >>> print ssh_run('sudo /usr/local/plone-5.1/secondary/scripts/restart_clients.sh')
    Marking client 1 down for maintenance
    Restarting client 1
    secondary_zeoclient1: stopped
    Waiting a bit...
    secondary_zeoclient1: started
    Waiting a bit...
    Fetch site homepages to warm cache
      /Plone
      /Plone
    Marking client 1 available
    <BLANKLINE>
    Waiting between clients
    <BLANKLINE>
    Marking client 2 down for maintenance
    Restarting client 2
    secondary_zeoclient2: stopped
    Waiting a bit...
    secondary_zeoclient2: started
    Waiting a bit...
    Fetch site homepages to warm cache
      /Plone
      /Plone
    Marking client 2 available
    Purging varnish cache
      /Plone
    <BLANKLINE>
      /Plone
    <BLANKLINE>
    Done

    >>> print ssh_run('curl --ipv4 -I -s http://localhost/logo.png | egrep "^(HTTP|Content-Type|X-Varnish-Cache)"')
    HTTP/1.1 200 OK
    Content-Type: image/png
    X-Varnish-Cache: MISS

This instance should be using a cronjob for hot restarts.

    >>> print ssh_run('grep "secondary" /etc/cron.d/restart_hot_plones')
    #Ansible: Restart secondary clients if too hot
    15,45 * * * * root /usr/local/plone-5.1/secondary/scripts/restart_if_hot.py

And, not memmon.

    >>> print ssh_run('grep memmon /etc/supervisor/conf.d/secondary_zeo.conf; echo $?')
    1

The load balancer for this instance should not be running health checks.

    >>> print ssh_run('grep "7081" /etc/haproxy/haproxy.cfg')
        server client1 127.0.0.1:7081 check port 7181

    >>> print >> sys.stderr, "Tests done"
